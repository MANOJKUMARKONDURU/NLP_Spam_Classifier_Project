# ğŸ›¡ï¸ NLP-Spam-Classifier-Project

> An end-to-end NLP project that classifies SMS/Email messages as Spam or Ham. Built with Python, NLTK &amp; Scikit-learn using Tokenization, Stemming, TF-IDF Vectorization and Multinomial Naive Bayes. Achieves 97% accuracy &amp; 100% precision on 5,572 messages. Deployed as an interactive web app using Streamlit.

![Python](https://img.shields.io/badge/Python-3.8+-blue?style=flat-square&logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=flat-square&logo=streamlit&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white)
![NLTK](https://img.shields.io/badge/NLTK-154f3c?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

## ğŸŒ Live Demo

ğŸ‘‰ **[Try the App Here](https://nlpspamclassifierproject-xgtfrjue5lszz9wtlfy5b9.streamlit.app/)**

### ğŸ–¼ï¸ Screenshots

![App Interface](./screenshots/app-interface.png)
![Spam Detection Example](./screenshots/spam-example.png)
![Ham Example](./screenshots/ham-example.png)

---

## ğŸ“Œ Table of Contents

- [ğŸ“– Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸ“Š Exploratory Data Analysis](#-exploratory-data-analysis)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ NLP Pipeline](#-nlp-pipeline)
- [ğŸ“ˆ Model Performance](#-model-performance)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸš€ How to Run](#-how-to-run)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [âœ… Results](#-results)
- [ğŸ™Œ Acknowledgements](#-acknowledgements)

---

## ğŸ“– Overview

This is a complete end-to-end machine learning project that detects whether an SMS or Email message is **Spam** or **Ham (Not Spam)**. The project covers the entire ML workflow â€” from raw data cleaning and exploratory data analysis, to text preprocessing, model training, model comparison, and deployment as a web application.

The model was trained on the **SMS Spam Collection Dataset** containing 5,572 real-world messages and uses a **Multinomial Naive Bayes** classifier with **TF-IDF Vectorization** to achieve state-of-the-art results.

---

## âœ¨ Features

- âš¡ Real-time spam detection with confidence score
- ğŸ”¤ Complete NLP text preprocessing pipeline
- ğŸ¤– 10 machine learning algorithms compared
- ğŸ¨ Interactive dark-themed Streamlit web app ([see screenshot](./screenshots/app-interface.png))
- ğŸ•“ Prediction history tracking ([see screenshot](./screenshots/history.png))
- ğŸ’¡ Example spam and ham messages to test instantly
- â˜ï¸ Word cloud visualizations for spam and ham
- ğŸ”¢ Live character and word counter in the UI

---
## ğŸ“Š Exploratory Data Analysis
### ğŸ“¦ Dataset

| Property | Details |
|----------|---------|
| Name | SMS Spam Collection Dataset |
| Source | [UCI Machine Learning Repository](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) |
| Total Messages | 5,572 |
| Spam Messages | 747 (13.4%) |
| Ham Messages | 4,825 (86.6%) |
| Features | Message text |
| Target | spam / ham |

> âš ï¸ The dataset is imbalanced â€” 86.6% ham vs 13.4% spam. This makes **precision** a more important metric than accuracy to avoid false positives where a legitimate message is wrongly marked as spam.

### ğŸ”¥ Feature Correlation Heatmap ([Correlation Heatmap image](./screenshots/correlation-heatmap.png))

| Feature Pair | Correlation | Insight |
|--------------|-------------|---------|
| num_characters â†” num_words | 0.97 | ğŸ”´ Extreme multicollinearity |
| num_characters â†” message_type | 0.38 | ğŸŸ  Moderate predictor |
| num_words â†” num_sentences | 0.68 | ğŸŸ  Moderate correlation |
| num_sentences â†” message_type | 0.26 | âšª Weak predictor |

> **Decision:** Selected TF-IDF features (3,000 dimensions) over raw counts to avoid multicollinearity and capture semantic meaning.

### ğŸš¨ Top Spam Keywords ([Spam Word Cloud image](./screenshots/wordcloud-spam.png))

> **Most Common Spam Words:**
 - free, call, click, winner, prize, cash, urgent, claim, text, reply

**Pattern:** Action-oriented words designed to prompt immediate user response.

### âœ… Top Ham Keywords ([Ham Word Cloud Image](./screenshots/wordcloud-ham.png))

> **Most Common Ham Words:**
 - thanks, meeting, tomorrow, please, hello, time, work, day, good, see

 **Pattern:** Natural conversational language for legitimate communication.

### ğŸ“ˆ Top 30 Spam Keywords ([Bar Chart Image](./screenshots/top-spam-keywords.png))

> Frequency analysis shows which words are strongest spam indicators. Keywords like "call", "free", "winner" appear exclusively in spam messages.

---

## ğŸ“ Project Structure

```
nlp-spam-classifier/
â”‚
â”œâ”€â”€ app.py                  # Streamlit web application
â”œâ”€â”€ spam_detection.ipynb    # Full project notebook (EDA + Training)
â”œâ”€â”€ model.pkl               # Trained Multinomial Naive Bayes model
â”œâ”€â”€ vectorizer.pkl          # Fitted TF-IDF vectorizer
â”œâ”€â”€ spam.csv                # Dataset
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ”§ NLP Pipeline

Every message goes through this preprocessing pipeline before prediction:

```
ğŸ“© Raw Message
    â†“
1. ğŸ”¡ Lowercase conversion
    â†“
2. âœ‚ï¸  Tokenization             (nltk.word_tokenize)
    â†“
3. ğŸ”£ Remove special chars     (keep only alphanumeric)
    â†“
4. ğŸ›‘ Remove stop words        (nltk.corpus.stopwords)
    â†“
5. âŒ Remove punctuation       (string.punctuation)
    â†“
6. ğŸŒ± Stemming                 (PorterStemmer)
    â†“
7. ğŸ“ TF-IDF Vectorization     (max_features=3000)
    â†“
ğŸ¤– Multinomial Naive Bayes Prediction
```

**Example:**
```
Input  :  "FREE entry! Win a $1000 prize. Call NOW!!!"
Output :  "free entry win prize call"
```

---

## ğŸ“ˆ Model Performance

### ğŸ† All 10 Algorithms Compared

| Algorithm | Accuracy | Precision |
|-----------|----------|-----------|
| ğŸŒ² Extra Trees (ETC) | 98.36% | 100.00% |
| ğŸŒ³ Random Forest (RF) | 97.87% | 98.33% |
| â­ Multinomial NB | 97.58% | 100.00% |
| ğŸ”· SVC | 97.58% | 97.48% |
| ğŸ“‰ Logistic Regression | 95.84% | 96.12% |
| ğŸ’ Bagging Classifier | 95.74% | 84.06% |
| ğŸ“ˆ Gradient Boosting | 94.49% | 90.10% |
| ğŸŒ¿ Decision Tree | 93.33% | 87.10% |
| ğŸš€ AdaBoost | 91.88% | 86.49% |
| ğŸ“ K-Nearest Neighbors | 90.52% | 100.00% |

### ğŸ¤” Why Multinomial NB was chosen

Three models achieved 100% precision â€” ETC, KNN and Multinomial NB. Here is why MNB was the final choice:

| Model | Accuracy | Precision | Verdict |
|-------|----------|-----------|---------|
| Extra Trees | 98.36% | 100.00% | âŒ Heavy â€” uses 50 trees, slow to deploy |
| K-Nearest Neighbors | 90.52% | 100.00% | âŒ Low accuracy, slow on large data |
| **Multinomial NB** | **97.58%** | **100.00%** | âœ… Best balance â€” fast, accurate, lightweight |

- âœ… 100% Precision â€” zero false positives, no legitimate message marked as spam
- âœ… 97.58% Accuracy â€” significantly higher than KNN (90.52%)
- âœ… Lightweight and fast â€” ideal for real-time web deployment
- âœ… Best suited for text and NLP classification tasks

### ğŸ§ª 3 Naive Bayes Variants Compared

| Model | Accuracy | Precision |
|-------|----------|-----------|
| â­ Multinomial NB | 97.58% | 100.00% |
| Bernoulli NB | 98.33% | 99.24% |
| Gaussian NB | 87.63% | 71.76% |

---

## âš™ï¸ Installation

**1. Clone the repository**
```bash
git clone https://github.com/MANOJKUMARKONDURU/NLP_Spam_Classifier_Project.git
cd NLP_Spam_Classifier_Project
```

**2. Install dependencies**
```bash
pip install -r requirements.txt
```

**3. Download NLTK data**
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

---

## ğŸš€ How to Run

**Run the Streamlit app locally:**
```bash
streamlit run app.py
```

Then open your browser at `http://localhost:8501`

> ğŸ“ Make sure `model.pkl` and `vectorizer.pkl` are in the same directory as `app.py`. If not, run the full notebook first to generate them.

---

## ğŸ› ï¸ Tech Stack

| Category | Tools |
|----------|-------|
| ğŸ Language | Python 3.8+ |
| ğŸ”¤ NLP | NLTK, String |
| ğŸ¤– ML | Scikit-learn |
| ğŸ“ Vectorization | TF-IDF (max_features=3000) |
| ğŸ§  Model | Multinomial Naive Bayes |
| ğŸ“Š Visualization | Matplotlib, Seaborn, WordCloud |
| ğŸŒ Web App | Streamlit |
| ğŸ’¾ Serialization | Pickle |
| ğŸ““ Notebook | Google Colab |

---

## âœ… Results

```
ğŸ¤– Model        :  Multinomial Naive Bayes
ğŸ“ Vectorizer   :  TF-IDF (max_features=3000)
ğŸ¯ Accuracy     :  97.58%
âœ… Precision    :  100.00%
ğŸ§ª Test Size    :  20%  (1,114 messages)
ğŸ“š Train Size   :  80%  (4,458 messages)
ğŸ² Random State :  2
```

---

## ğŸ™Œ Acknowledgements

- ğŸ“¦ Dataset: [SMS Spam Collection â€” Kaggle / UCI](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)
- ğŸ”¤ NLTK for NLP preprocessing tools
- ğŸ¤– Scikit-learn for machine learning algorithms
- ğŸŒ Streamlit for the interactive web interface

---

<p align="center">Built with â¤ï¸ using Python Â· NLTK Â· Scikit-learn Â· Streamlit</p>
